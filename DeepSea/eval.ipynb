{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessior/anaconda3/envs/exploration-maze/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from run import agents, run_agent, AgentStats\n",
    "from numpy.typing import NDArray\n",
    "from typing import NamedTuple, Sequence, Tuple\n",
    "from deepsea import MultiRewardsDeepSea\n",
    "import multiprocessing as mp\n",
    "from scipy.stats import t as tstudent\n",
    "import torch\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "try:\n",
    "     set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "class Results(NamedTuple):\n",
    "    training_rewards: NDArray[np.float64]\n",
    "    greedy_rewards: Sequence[Tuple[int, NDArray[np.float64]]]\n",
    "    regret: NDArray[np.float64]\n",
    "    agent_stats: AgentStats\n",
    "    \n",
    "    \n",
    "def CE(x, c=0.95):\n",
    "    N = x.shape[0]\n",
    "    alpha = c + (1-c)/2\n",
    "    c = tstudent.ppf(alpha, N)\n",
    "    s = np.std(x, axis=0, ddof=1)\n",
    "    return x.mean(0), c * s/ np.sqrt(N)\n",
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent boot_dqn_torch - multi_rewards False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep. 999 - Regret: 993.412 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [05:12<00:00,  3.20it/s]s]   \n",
      "Ep. 999 - Regret: 993.839 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [05:12<00:00,  3.20it/s]s]\n",
      "Ep. 999 - Regret: 993.535 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [05:13<00:00,  3.19it/s]s]\n",
      "Ep. 999 - Regret: 993.326 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [05:14<00:00,  3.18it/s]  \n",
      "Ep. 999 - Regret: 973.974 - Last 10 ep. avg ret. 0.692 - Last greedy avg. ret. 0.991 (std 0.0): 100%|██████████| 1000/1000 [05:15<00:00,  3.17it/s]\n",
      "Ep. 999 - Regret: 474.082 - Last 10 ep. avg ret. 0.99 - Last greedy avg. ret. 0.99 (std 0.0): 100%|██████████| 1000/1000 [05:15<00:00,  3.17it/s]/s]\n",
      "Ep. 999 - Regret: 993.375 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [05:15<00:00,  3.17it/s]\n",
      "Ep. 999 - Regret: 993.602 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [05:16<00:00,  3.16it/s]\n",
      "Ep. 999 - Regret: 993.836 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [05:09<00:00,  3.23it/s]\n",
      "Ep. 999 - Regret: 993.617 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [05:10<00:00,  3.23it/s]  \n",
      "Ep. 999 - Regret: 478.194 - Last 10 ep. avg ret. 0.99 - Last greedy avg. ret. 0.99 (std 0.0): 100%|██████████| 1000/1000 [05:09<00:00,  3.23it/s]/s]\n",
      "Ep. 999 - Regret: 993.61 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [05:11<00:00,  3.21it/s]\n",
      "Ep. 999 - Regret: 993.87 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [05:09<00:00,  3.23it/s]   \n",
      "Ep. 999 - Regret: 993.679 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [05:10<00:00,  3.22it/s]  \n",
      "Ep. 999 - Regret: 993.592 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [05:10<00:00,  3.22it/s]\n",
      "Ep. 999 - Regret: 993.654 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [05:08<00:00,  3.24it/s]  \n",
      "Ep. 3 - Regret: 3.983 - Last 10 ep. avg ret. -0.005 - Last greedy avg. ret. 0 (std 0):   0%|          | 0/1000 [00:00<?, ?it/s]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent explorative_generative_off_policy - multi_rewards False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep. 999 - Regret: 993.898 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.004 (std 0.0): 100%|██████████| 1000/1000 [11:23<00:00,  1.46it/s]\n",
      "Ep. 999 - Regret: 993.797 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. 0.0 (std 0.0): 100%|██████████| 1000/1000 [11:24<00:00,  1.46it/s]  \n",
      "Ep. 999 - Regret: 993.743 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.003 (std 0.0): 100%|██████████| 1000/1000 [11:27<00:00,  1.45it/s]\n",
      "Ep. 999 - Regret: 993.846 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [11:27<00:00,  1.45it/s]\n",
      "Ep. 999 - Regret: 993.935 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [11:28<00:00,  1.45it/s]\n",
      "Ep. 999 - Regret: 684.667 - Last 10 ep. avg ret. 0.99 - Last greedy avg. ret. 0.99 (std 0.0): 100%|██████████| 1000/1000 [11:29<00:00,  1.45it/s]\n",
      "Ep. 999 - Regret: 993.828 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [11:29<00:00,  1.45it/s]\n",
      "Ep. 999 - Regret: 393.711 - Last 10 ep. avg ret. 0.99 - Last greedy avg. ret. 0.99 (std 0.0): 100%|██████████| 1000/1000 [11:31<00:00,  1.45it/s]\n",
      "Ep. 886 - Regret: 881.418 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.001 (std 0.0):  89%|████████▊ | 887/1000 [10:11<01:16,  1.48it/s]/home/alessior/rl-exploration-maze/DeepSea/agents/explorative_generative_off_policy.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  p = (H/H.sum(-1, keepdims=True))\n",
      "Ep. 877 - Regret: 872.77 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. 0.0 (std 0.0):  88%|████████▊ | 878/1000 [10:06<01:24,  1.45it/s]\n",
      "Ep. 999 - Regret: 535.32 - Last 10 ep. avg ret. 0.991 - Last greedy avg. ret. 0.991 (std 0.0): 100%|██████████| 1000/1000 [11:19<00:00,  1.47it/s]s]\n",
      "Ep. 999 - Regret: 993.513 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.003 (std 0.0): 100%|██████████| 1000/1000 [11:20<00:00,  1.47it/s]\n",
      "Ep. 999 - Regret: 993.802 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [11:19<00:00,  1.47it/s]\n",
      "Ep. 999 - Regret: 990.993 - Last 10 ep. avg ret. -0.005 - Last greedy avg. ret. -0.003 (std 0.0): 100%|██████████| 1000/1000 [11:20<00:00,  1.47it/s]\n",
      "Ep. 999 - Regret: 993.791 - Last 10 ep. avg ret. -0.001 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [11:19<00:00,  1.47it/s]\n",
      "Ep. 999 - Regret: 993.756 - Last 10 ep. avg ret. -0.002 - Last greedy avg. ret. -0.001 (std 0.0): 100%|██████████| 1000/1000 [11:18<00:00,  1.47it/s]\n",
      "Ep. 999 - Regret: 522.557 - Last 10 ep. avg ret. 0.991 - Last greedy avg. ret. 0.991 (std 0.0): 100%|██████████| 1000/1000 [11:22<00:00,  1.46it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/alessior/anaconda3/envs/exploration-maze/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/alessior/anaconda3/envs/exploration-maze/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/home/alessior/rl-exploration-maze/DeepSea/run.py\", line 25, in run_agent\n    seed=seed)\n  File \"/home/alessior/rl-exploration-maze/DeepSea/run.py\", line 98, in run\n  File \"/home/alessior/rl-exploration-maze/DeepSea/agents/explorative_generative_off_policy.py\", line 266, in select_action\n    return self._select_action(observation)\n  File \"/home/alessior/anaconda3/envs/exploration-maze/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/alessior/rl-exploration-maze/DeepSea/agents/explorative_generative_off_policy.py\", line 263, in _select_action\n    return np.random.choice(self._num_actions, p=p)\n  File \"mtrand.pyx\", line 935, in numpy.random.mtrand.RandomState.choice\nValueError: probabilities contain NaN\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboot_dqn_torch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplorative_generative_off_policy\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - multi_rewards \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmulti_rewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     training_rewards, greedy_rewards, regret, stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_REWARD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSLIPPING_PROBABILITY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPISODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFREQ_EVAL_GREEDY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EVAL_GREEDY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUM_RUNS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m     data[key_dict][agent_name] \u001b[38;5;241m=\u001b[39m Results(training_rewards, greedy_rewards, regret, stats)\n",
      "File \u001b[0;32m~/anaconda3/envs/exploration-maze/lib/python3.10/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/exploration-maze/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "N_EPISODES = 1000\n",
    "FREQ_EVAL_GREEDY = 100\n",
    "NUM_EVAL_GREEDY = 20\n",
    "NUM_PROC = 4\n",
    "NUM_RUNS = 16\n",
    "SIZE = 10\n",
    "MAX_REWARD = 1\n",
    "SLIPPING_PROBABILITY = 0.\n",
    "\n",
    "# agent_name: str, seed: int, multi_rewards: bool, size: int, max_reward: float, slipping_probability: float,\n",
    "#                 num_episodes: int, freq_val_greedy: int, num_eval_greedy:int):\n",
    "\n",
    "with Pool(NUM_PROC) as pool:\n",
    "    for multi_rewards in [False, True]:\n",
    "        key_dict = f'multi_rewards_enabled_{multi_rewards}'\n",
    "        if key_dict not in data:\n",
    "            data[key_dict] = {}\n",
    "        for agent_name in ['boot_dqn_torch', 'ids', 'explorative_generative_off_policy']:\n",
    "            print(f'Running agent {agent_name} - multi_rewards {multi_rewards}')\n",
    "            training_rewards, greedy_rewards, regret, stats = zip(*pool.starmap(\n",
    "                run_agent, [(agent_name, idx, multi_rewards, SIZE, MAX_REWARD, SLIPPING_PROBABILITY, N_EPISODES, FREQ_EVAL_GREEDY, NUM_EVAL_GREEDY) for idx in range(NUM_RUNS)]))\n",
    "            data[key_dict][agent_name] = Results(training_rewards, greedy_rewards, regret, stats)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = {\n",
    "    True: '--',\n",
    "    False: '-'\n",
    "}\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "for multi_rewards in [False, True]:\n",
    "    for agent_name in ['boot_dqn_torch','explorative_generative_off_policy']:\n",
    "        agent_data = data[f'multi_rewards_enabled_{multi_rewards}'][agent_name]\n",
    "        mu, ce = CE(np.array(agent_data.regret))\n",
    "\n",
    "        t = np.arange(N_EPISODES + 1)\n",
    "        ax[0].plot(t, mu, label=f'{agent_name} - Multi rew. {multi_rewards}', linestyle=styles[multi_rewards])\n",
    "        ax[0].fill_between(t, np.clip(mu-ce,0,np.infty), mu+ce, alpha=0.2)\n",
    "        \n",
    "        greedy_data = np.array([list(zip(*agent_data.greedy_rewards[x]))[1] for x in range(NUM_RUNS)])\n",
    "\n",
    "        mu, ce = CE(greedy_data.swapaxes(1,2).reshape(-1, greedy_data.shape[1]))\n",
    "        x = range(len(mu))\n",
    "        ax[1].plot(x, mu, linestyle=styles[multi_rewards])\n",
    "        ax[1].fill_between(x, np.clip(mu-ce,0, np.infty), mu+ce, alpha=0.2)\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "ax[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_data = np.array([list(zip(*data['multi_rewards_enabled_False']['boot_dqn_torch'].greedy_rewards[x]))[1] for x in range(NUM_RUNS)])\n",
    "\n",
    "mu_groups, std_groups = greedy_data.mean(-1), greedy_data.std(-1, ddof=1)\n",
    "RSS, ESS = std_groups.mean(0), mu_groups.std(0, ddof=1)\n",
    "mu, std = mu_groups.mean(0), RSS+ESS\n",
    "\n",
    "\n",
    "alpha = .95 + (1-.95)/2\n",
    "c = tstudent.ppf(alpha, 5)\n",
    "se = c * std/ np.sqrt(5)\n",
    "\n",
    "x = range(len(mu))\n",
    "plt.plot(x, mu)\n",
    "plt.fill_between(x, np.clip(mu-se,0, np.infty), mu+se, alpha=0.2)\n",
    "\n",
    "#plt.plot(x, ESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('exploration-maze')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e55163f971ebf1a2ef620dd9eaffe72e4e29cde2845dd2a5421b0d103c3dfa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
